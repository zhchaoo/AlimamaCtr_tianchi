{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels as sm\n",
    "import matplotlib.pylab as plt\n",
    "import config as cf\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from chinese_calendar import is_workday, is_holiday\n",
    "from jupyterthemes import jtplot\n",
    "from util import timeit\n",
    "from joblib import Parallel, delayed\n",
    "from IPython.core.display import clear_output\n",
    "\n",
    "\n",
    "jtplot.style()\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(cf.round1_train_file_path, sep = ' ')\n",
    "test_a_df = pd.read_csv(cf.round1_test_a_file_path, sep = ' ')\n",
    "test_b_df = pd.read_csv(cf.round1_test_b_file_path, sep = ' ')\n",
    "test_df = test_a_df.append(test_b_df)\n",
    "\n",
    "category_df = train_df['item_category_list'].unique()\n",
    "category_ids = pd.DataFrame({'item_category_list' : category_df, 'item_category_id' : np.arange(len(category_df))})\n",
    "train_df = train_df.merge(category_ids, on='item_category_list')\n",
    "test_df = test_df.merge(category_ids, on='item_category_list')\n",
    "\n",
    "time_offset = 8 * 60 * 60 - 365 * 24 * 60 * 60\n",
    "train_df.loc[:,'context_datetime'] = pd.to_datetime(train_df.loc[:,'context_timestamp'] + time_offset, unit='s')\n",
    "test_df.loc[:,'context_datetime'] = pd.to_datetime(test_df.loc[:,'context_timestamp'] + time_offset, unit='s')\n",
    "train_df.loc[:,'context_day'] = train_df.loc[:,'context_datetime'].map(lambda x:x.day)\n",
    "test_df.loc[:,'context_day'] = test_df.loc[:,'context_datetime'].map(lambda x:x.day)\n",
    "train_df.loc[:,'context_hour'] = train_df.loc[:,'context_datetime'].map(lambda x:x.hour)\n",
    "test_df.loc[:,'context_hour'] = test_df.loc[:,'context_datetime'].map(lambda x:x.hour)\n",
    "train_df.loc[:,'user_age_level2'] = train_df.loc[:, 'user_age_level'].apply(lambda x: 1 if x == 1004 or x == 1005 or x == 1006 or x == 1007  else 2)\n",
    "test_df.loc[:,'user_age_level2'] = test_df.loc[:, 'user_age_level'].apply(lambda x: 1 if x == 1004 or x == 1005 or x == 1006 or x == 1007  else 2)\n",
    "train_df.loc[:,'user_star_level2'] = train_df.loc[:, 'user_star_level'].apply(lambda x: 1 if x == -1 or x == 3000 or x == 3001 else 3 if x == 3010 else 2)\n",
    "test_df.loc[:,'user_star_level2'] = test_df.loc[:, 'user_star_level'].apply(lambda x: 1 if x == -1 or x == 3000 or x == 3001 else 3 if x == 3010 else 2)\n",
    "# train_df.loc[:,'context_minustamp'] = train_df.loc[:,'context_datetime'].map(lambda x:x.hour * 60 + x.minute)\n",
    "# test_df.loc[:,'context_minustamp'] = test_df.loc[:,'context_datetime'].map(lambda x:x.hour * 60 + x.minute)\n",
    "\n",
    "stat_df = train_df\n",
    "all_df = pd.concat([train_df, test_df]).drop_duplicates(subset='instance_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(df, name_list):\n",
    "    for name in name_list:\n",
    "        # 归一化\n",
    "        max_number = df[name].max()\n",
    "        min_number = df[name].min()\n",
    "        # assert max_number != min_number, 'max == min in COLUMN {0}'.format(name)\n",
    "        df.loc[:,name] = df.loc[:,name].map(lambda x: float(x - min_number + 0.001) / float(max_number - min_number + 0.001))\n",
    "        # 做简单的平滑,试试效果如何\n",
    "    return df\n",
    "\n",
    "def min_max_normalize_log(df, name_list):\n",
    "    for name in name_list:\n",
    "        # 归一化\n",
    "        max_number = df[name].max()\n",
    "        min_number = df[name].min()\n",
    "        # assert max_number != min_number, 'max == min in COLUMN {0}'.format(name)\n",
    "        df.loc[:,name] = df.loc[:,name].map(lambda x: np.log(x + 1) / np.log(max_number + 1))\n",
    "        # 做简单的平滑,试试效果如何\n",
    "    return df\n",
    "\n",
    "def normalize_log(df, name_list):\n",
    "    for name in name_list:\n",
    "        df.loc[:,name] = df.loc[:,name].map(lambda x: np.log(x + 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立基础特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_base_feature(df):\n",
    "    feature_list = []\n",
    "    if 'is_trade' in df:\n",
    "        feature_list.append('is_trade')\n",
    "    feature_list.extend(['instance_id', 'context_id', 'context_timestamp', 'context_day', 'item_property_list'])\n",
    "    feature_list.extend(['user_id', 'item_id', 'shop_id', 'item_brand_id', 'item_city_id', 'item_category_id'])\n",
    "    feature_list.extend(['item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level',\n",
    "                         'user_gender_id', 'user_age_level', 'user_age_level2', 'user_occupation_id',\n",
    "                         'user_star_level', 'user_star_level2', 'context_page_id', 'shop_review_num_level', 'shop_star_level',\n",
    "                         'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description'])\n",
    "    base_feature = df[feature_list]\n",
    "    return base_feature\n",
    "\n",
    "def gen_base_combine_feature(df):\n",
    "    base_df = df[['item_id', 'user_id', 'shop_id',\n",
    "                 'item_sales_level', 'item_price_level', 'item_collected_level', 'item_pv_level',\n",
    "                 'user_gender_id', 'user_age_level', 'user_star_level', 'user_occupation_id',\n",
    "                 'shop_review_num_level', 'shop_star_level']].drop_duplicates()\n",
    "    combine_list = {\n",
    "        'price_sale': ['item_sales_level', 'item_price_level'],\n",
    "        'collect_sale': ['item_sales_level', 'item_collected_level'],\n",
    "        'collect_price': ['item_price_level', 'item_collected_level'],\n",
    "        'collect_pv': ['item_pv_level', 'item_collected_level'],\n",
    "        'sale_pv': ['item_pv_level', 'item_sales_level'],\n",
    "        \n",
    "        'gender_age': ['user_gender_id', 'user_age_level'],\n",
    "        'gender_occ': ['user_gender_id', 'user_occupation_id'],\n",
    "        'gender_star': ['user_gender_id', 'user_star_level'],\n",
    "        'review_star': ['shop_review_num_level', 'shop_star_level'],\n",
    "        \n",
    "        'price_gender': ['user_gender_id', 'item_price_level'],\n",
    "        'price_occ': ['item_price_level', 'user_occupation_id'],\n",
    "        'price_star': ['item_price_level', 'user_star_level'],\n",
    "    }\n",
    "    for key, combine in combine_list.items():\n",
    "        base_df.loc[:,key] = base_df[combine[0]] * 24 + base_df[combine[1]]\n",
    "    \n",
    "    return base_df\n",
    "\n",
    "\n",
    "def process_intersection_id(train_df, test_df, stat_df = None, all_df = None):\n",
    "    columns = ['user_id', 'item_id', 'shop_id', 'item_brand_id']\n",
    "    ret_tr_df = train_df[columns].drop_duplicates()\n",
    "    ret_te_df = test_df[columns].drop_duplicates()\n",
    "    \n",
    "    for column in columns:\n",
    "        a = set(ret_tr_df[column].tolist())\n",
    "        b = set(ret_te_df[column].tolist())\n",
    "        is_id = a & b\n",
    "        tr_df = ret_tr_df.loc[ret_tr_df[column].isin(is_id)][[column]]\n",
    "        tr_df.loc[:, column + '_is'] = 1\n",
    "        te_df = ret_te_df.loc[ret_te_df[column].isin(is_id)][[column]]\n",
    "        te_df.loc[:, column + '_is'] = 1\n",
    "        ret_tr_df = ret_tr_df.merge(tr_df.drop_duplicates(), how='left')\n",
    "        ret_te_df = ret_te_df.merge(te_df.drop_duplicates(), how='left')\n",
    "    \n",
    "    return ret_tr_df.fillna(-1).astype('int64'), ret_te_df.fillna(-1).astype('int64')\n",
    "\n",
    "    \n",
    "def process_base_feature(train_df, test_df, stat_df=None, all_df = None):\n",
    "    tr_df = gen_base_feature(train_df)\n",
    "    te_df = gen_base_feature(test_df)\n",
    "    return tr_df, te_df\n",
    "\n",
    "def process_base_combine_feature(train_df, test_df, stat_df=None, all_df = None):\n",
    "    return map(gen_base_combine_feature, (train_df, test_df))\n",
    "\n",
    "# train_base_ft, test_base_ft = process_base_feature(train_df, test_df)\n",
    "# train_base_ft1, test_base_ft1 = process_base_combine_feature(train_df, test_df)\n",
    "# train_id_is, test_id_is = process_intersection_id(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立用户特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gen_user_ot_feature(stat_df, extend_days):\n",
    "    stat_user_df = stat_df[['user_id', 'context_day', 'context_hour', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_user_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        dfs = {'all':stat_user_df.loc[stat_user_df['context_day'] != day],\n",
    "               'last':stat_user_df.loc[stat_user_df['context_day'] == day - 1]\n",
    "              }\n",
    "        user_features = dfs['all'].drop(columns=['is_trade']).drop_duplicates()\n",
    "        user_features.loc[:, 'context_day'] = day\n",
    "        groupbys = {'u' : ['user_id'], \n",
    "                    'uh' : ['user_id', 'context_hour']}\n",
    "\n",
    "        for key, groupby in groupbys.items():\n",
    "            for dkey, user_df in dfs.items():\n",
    "                if user_df.empty:\n",
    "                    continue\n",
    "                key = dkey + '_' + key\n",
    "                trade_cnt_key = 'utrade_' + key + '_cnt'\n",
    "                query_cnt_key = 'uquery_' + key + '_cnt'\n",
    "                user_rate = user_df[groupby + ['is_trade']].rename(columns={'is_trade':trade_cnt_key})\n",
    "                user_rate.loc[:, query_cnt_key] = 1\n",
    "                user_rate = user_rate.groupby(groupby, as_index=False).sum()\n",
    "                user_rate.loc[:, 'urate_' + key] = user_rate[trade_cnt_key] / user_rate[query_cnt_key]\n",
    "                user_features = user_features.merge(user_rate, how='left')\n",
    "\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), user_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            user_features.loc[:,cnt_columns] = user_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        user_features = min_max_normalize_log(user_features, cnt_columns)\n",
    "        feature_frames.append(user_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "\n",
    "def gen_user_prob_feature(stat_df, columns):\n",
    "    data = stat_df[['instance_id'] + columns]\n",
    "    print('用户有多少性别')\n",
    "    itemcnt = data.groupby(['user_id'], as_index=False)['instance_id'].agg({'user_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['user_id'], how='left')\n",
    "\n",
    "    for col in ['user_gender_id','user_age_level', 'user_occupation_id', 'user_star_level']:\n",
    "        itemcnt = data.groupby([col, 'user_id'], as_index=False)['instance_id'].agg({str(col) + '_user_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'user_id'], how='left')\n",
    "        data[str(col) + '_user_prob']=data[str(col) + '_user_cnt']/data['user_cnt']\n",
    "    del data['user_cnt']\n",
    "\n",
    "    print('性别的年龄段，职业有多少')\n",
    "    itemcnt = data.groupby(['user_gender_id'], as_index=False)['instance_id'].agg({'user_gender_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['user_gender_id'], how='left')\n",
    "\n",
    "    for col in ['user_age_level', 'user_occupation_id', 'user_star_level']:\n",
    "        itemcnt = data.groupby([col, 'user_gender_id'], as_index=False)['instance_id'].agg({str(col) + '_user_gender_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'user_gender_id'], how='left')\n",
    "        data[str(col) + '_user_gender_prob']=data[str(col) + '_user_gender_cnt']/data['user_gender_cnt']\n",
    "    del data['user_gender_cnt']\n",
    "\n",
    "    print('user_age_level对应的user_occupation_id，user_star_level')\n",
    "    itemcnt = data.groupby(['user_age_level'], as_index=False)['instance_id'].agg({'user_age_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['user_age_level'], how='left')\n",
    "\n",
    "    for col in ['user_occupation_id', 'user_star_level']:\n",
    "        itemcnt = data.groupby([col, 'user_age_level'], as_index=False)['instance_id'].agg({str(col) + '_user_age_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'user_age_level'], how='left')\n",
    "        data[str(col) + '_user_age_prob']=data[str(col) + '_user_age_cnt']/data['user_age_cnt']\n",
    "    del data['user_age_cnt']\n",
    "\n",
    "    print('user_occupation_id对应的user_star_level')\n",
    "    itemcnt = data.groupby(['user_occupation_id'], as_index=False)['instance_id'].agg({'user_occ_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['user_occupation_id'], how='left')\n",
    "    for col in ['user_star_level']:\n",
    "        itemcnt = data.groupby([col, 'user_occupation_id'], as_index=False)['instance_id'].agg({str(col) + '_user_occ_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'user_occupation_id'], how='left')\n",
    "        data[str(col) + '_user_occ_prob']=data[str(col) + '_user_occ_cnt']/data['user_occ_cnt']\n",
    "    del data['user_occ_cnt']\n",
    "    \n",
    "    del data['instance_id']\n",
    "    \n",
    "    cnt_columns = filter(lambda x:x.endswith('_cnt'), data.columns.values)\n",
    "    data = min_max_normalize(data, cnt_columns)\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_user_ot_feature(train_df, test_df, stat_df, all_df = None):\n",
    "    stat_user_df = gen_user_ot_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['user_id', 'context_day', 'context_hour']\n",
    "    return map(lambda df:df[base_columns].merge(stat_user_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "\n",
    "def process_user_prob_feature(train_df, test_df, stat_df, all_df):\n",
    "    base_columns = ['user_id', 'user_gender_id', 'user_age_level', 'user_occupation_id', 'user_star_level']\n",
    "    stat_user_df = gen_user_prob_feature(all_df, base_columns).drop_duplicates()\n",
    "    return map(lambda df:df[base_columns].merge(stat_user_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "\n",
    "# train_user_ct1, test_user_ct1 = process_user_ot_feature(train_df, test_df, stat_df)\n",
    "# train_user_ct2, test_user_ct2 = process_user_prob_feature(train_df, test_df, stat_df, all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立用户-商品特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     39,
     153
    ]
   },
   "outputs": [],
   "source": [
    "def gen_user_item_ot_feature(stat_df, extend_days):\n",
    "    stat_user_item_df = stat_df[['user_id', 'shop_id', 'item_id', 'item_brand_id', 'item_category_id', 'item_city_id', 'item_price_level', 'context_day', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_user_item_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        dfs = {'all':stat_user_item_df.loc[stat_user_item_df['context_day'] != day],\n",
    "               'last':stat_user_item_df.loc[stat_user_item_df['context_day'] == day - 1]\n",
    "              }\n",
    "        user_item_features = dfs['all'].drop(columns=['is_trade']).drop_duplicates()\n",
    "        user_item_features.loc[:, 'context_day'] = day\n",
    "        groupbys = {'us' : ['user_id', 'shop_id'],\n",
    "                    'ui' : ['user_id', 'item_id'], \n",
    "                    'ub' : ['user_id', 'item_brand_id'], \n",
    "                    'ucg' : ['user_id', 'item_category_id'],\n",
    "                    'uct' : ['user_id', 'item_city_id'],\n",
    "                    'uip' : ['user_id', 'item_price_level']}\n",
    "\n",
    "        for key, groupby in groupbys.items():\n",
    "            for dkey, user_item_df in dfs.items():\n",
    "                if user_item_df.empty:\n",
    "                    continue\n",
    "                key = dkey + '_' + key\n",
    "                trade_cnt_key = 'uitrade_' + key + '_cnt'\n",
    "                query_cnt_key = 'uiquery_' + key + '_cnt'\n",
    "                user_item_rate = user_item_df[groupby + ['is_trade']].rename(columns={'is_trade':trade_cnt_key})\n",
    "                user_item_rate.loc[:, query_cnt_key] = 1\n",
    "                user_item_rate = user_item_rate.groupby(groupby, as_index=False).sum()\n",
    "                user_item_rate.loc[:, 'uirate_' + key] = user_item_rate[trade_cnt_key] / user_item_rate[query_cnt_key]\n",
    "                user_item_features = user_item_features.merge(user_item_rate, how='left')\n",
    "\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), user_item_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            user_item_features.loc[:,cnt_columns] = user_item_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        user_item_features = min_max_normalize_log(user_item_features, cnt_columns)\n",
    "        feature_frames.append(user_item_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "\n",
    "def gen_user_item_prob_feature(stat_df, columns):\n",
    "    data = stat_df[['instance_id'] + columns]\n",
    "    itemcnt = data.groupby(['user_id'], as_index=False)['instance_id'].agg({'user_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['user_id'], how='left')\n",
    "    print('一个user有多少item_id,item_brand_id……')\n",
    "    for col in ['item_id',\n",
    "                'item_brand_id','item_city_id','item_price_level',\n",
    "                'item_sales_level','item_collected_level','item_pv_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'user_id'], as_index=False)['instance_id'].agg({str(col)+'_user_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'user_id'], how='left')\n",
    "        data[str(col) + '_user_prob'] = data[str(col) + '_user_cnt'] / data['user_cnt']\n",
    "\n",
    "    print('一个user_gender有多少item_id,item_brand_id……')\n",
    "    itemcnt = data.groupby(['user_gender_id'], as_index=False)['instance_id'].agg({'user_gender_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['user_gender_id'], how='left')\n",
    "    for col in ['item_id',\n",
    "                'item_brand_id','item_city_id','item_price_level',\n",
    "                'item_sales_level','item_collected_level','item_pv_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'user_gender_id'], as_index=False)['instance_id'].agg({str(col)+'_user_gender_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'user_gender_id'], how='left')\n",
    "        data[str(col) + '_user_gender_prob'] = data[str(col) + '_user_gender_cnt'] / data['user_gender_cnt']\n",
    "\n",
    "    print('一个user_age_level有多少item_id,item_brand_id……')\n",
    "    itemcnt = data.groupby(['user_age_level'], as_index=False)['instance_id'].agg({'user_age_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['user_age_level'], how='left')\n",
    "    for col in ['item_id',\n",
    "                'item_brand_id','item_city_id','item_price_level',\n",
    "                'item_sales_level','item_collected_level','item_pv_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'user_age_level'], as_index=False)['instance_id'].agg({str(col)+'_user_age_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'user_age_level'], how='left')\n",
    "        data[str(col) + '_user_age_prob'] = data[str(col) + '_user_age_cnt'] / data['user_age_cnt']\n",
    "\n",
    "    print('一个user_occupation_id有多少item_id,item_brand_id…')\n",
    "    itemcnt = data.groupby(['user_occupation_id'], as_index=False)['instance_id'].agg({'user_occ_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['user_occupation_id'], how='left')\n",
    "    for col in ['item_id',\n",
    "                'item_brand_id','item_city_id','item_price_level',\n",
    "                'item_sales_level','item_collected_level','item_pv_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'user_occupation_id'], as_index=False)['instance_id'].agg({str(col)+'_user_occ_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'user_occupation_id'], how='left')\n",
    "        data[str(col) + '_user_occ_prob'] = data[str(col) + '_user_occ_cnt'] / data['user_occ_cnt']\n",
    "       \n",
    "\n",
    "    # user_shop\n",
    "    print('一个user有多少shop_id,shop_review_num_level……')\n",
    "\n",
    "    for col in ['shop_id', 'shop_review_num_level', 'shop_star_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'user_id'], as_index=False)['instance_id'].agg(\n",
    "            {str(col) + '_user_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'user_id'], how='left')\n",
    "        data[str(col) + '_user_prob'] = data[str(col) + '_user_cnt'] / data['user_cnt']\n",
    "    del data['user_cnt']\n",
    "\n",
    "    print('一个user_gender有多少shop_id,shop_review_num_level……')\n",
    "    for col in ['shop_id', 'shop_review_num_level', 'shop_star_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'user_gender_id'], as_index=False)['instance_id'].agg(\n",
    "            {str(col) + '_user_gender_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'user_gender_id'], how='left')\n",
    "        data[str(col) + '_user_gender_prob'] = data[str(col) + '_user_gender_cnt'] / data['user_gender_cnt']\n",
    "    del data['user_gender_cnt']\n",
    "\n",
    "    print('一个user_age_level有多少shop_id,shop_review_num_level……')\n",
    "    for col in ['shop_id', 'shop_review_num_level', 'shop_star_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'user_age_level'], as_index=False)['instance_id'].agg(\n",
    "            {str(col) + '_user_age_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'user_age_level'], how='left')\n",
    "        data[str(col) + '_user_age_prob'] = data[str(col) + '_user_age_cnt'] / data['user_age_cnt']\n",
    "    del data['user_age_cnt']\n",
    "\n",
    "    print('一个user_occupation_id有多少shop_id,shop_review_num_level……')\n",
    "    for col in ['shop_id', 'shop_review_num_level', 'shop_star_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'user_occupation_id'], as_index=False)['instance_id'].agg(\n",
    "            {str(col) + '_user_occ_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'user_occupation_id'], how='left')\n",
    "        data[str(col) + '_user_occ_prob'] = data[str(col) + '_user_occ_cnt'] / data['user_occ_cnt']\n",
    "    del data['user_occ_cnt']\n",
    "\n",
    "    \n",
    "    # shop_item\n",
    "    print('一个shop有多少item_id,item_brand_id,item_city_id,item_price_level……')\n",
    "    itemcnt = data.groupby(['shop_id'], as_index=False)['instance_id'].agg({'shop_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['shop_id'], how='left')\n",
    "    for col in ['item_id',\n",
    "                'item_brand_id','item_city_id','item_price_level',\n",
    "                'item_sales_level','item_collected_level','item_pv_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'shop_id'], as_index=False)['instance_id'].agg({str(col)+'_shop_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'shop_id'], how='left')\n",
    "        data[str(col) + '_shop_prob'] = data[str(col) + '_shop_cnt'] / data['shop_cnt']\n",
    "    del data['shop_cnt']\n",
    "\n",
    "    print('一个shop_review_num_level有多少item_id,item_brand_id,item_city_id,item_price_level……')\n",
    "    itemcnt = data.groupby(['shop_review_num_level'], as_index=False)['instance_id'].agg({'shop_rev_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['shop_review_num_level'], how='left')\n",
    "    for col in ['item_id',\n",
    "                'item_brand_id','item_city_id','item_price_level',\n",
    "                'item_sales_level','item_collected_level','item_pv_level']:\n",
    "        item_shop_cnt = data.groupby([col, 'shop_review_num_level'], as_index=False)['instance_id'].agg({str(col)+'_shop_rev_cnt': 'count'})\n",
    "        data = pd.merge(data, item_shop_cnt, on=[col, 'shop_review_num_level'], how='left')\n",
    "        data[str(col) + '_shop_rev_prob'] = data[str(col) + '_shop_rev_cnt'] / data['shop_rev_cnt']\n",
    "    del data['shop_rev_cnt']\n",
    "\n",
    "    del data['instance_id']\n",
    "    \n",
    "    cnt_columns = filter(lambda x:x.endswith('_cnt'), data.columns.values)\n",
    "    data = min_max_normalize(data, cnt_columns)\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_user_item_ot_feature(train_df, test_df, stat_df, all_df = None):\n",
    "    stat_user_item_df = gen_user_item_ot_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['user_id', 'shop_id', 'item_id', 'item_brand_id', 'item_category_id', 'context_day', 'item_city_id', 'item_price_level']\n",
    "    return map(lambda df:df[base_columns].merge(stat_user_item_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "\n",
    "def process_user_item_prob_feature(train_df, test_df, stat_df, all_df):\n",
    "    base_columns = ['user_id', 'item_id', 'item_brand_id','item_city_id','item_price_level', 'user_age_level', 'shop_id', 'shop_star_level',\n",
    "                    'user_gender_id', 'user_occupation_id', 'item_sales_level','item_collected_level','item_pv_level', 'shop_review_num_level']\n",
    "    stat_user_item_df = gen_user_item_prob_feature(all_df, base_columns).drop_duplicates()\n",
    "    return map(lambda df:df[base_columns].merge(stat_user_item_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "# train_user_ft, test_user_ft = process_user_item_ot_feature(train_df, test_df, stat_df)\n",
    "# train_user_ft, test_user_ft = process_user_item_prob_feature(train_df, test_df, stat_df, all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立商品特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     44,
     52,
     122,
     132
    ]
   },
   "outputs": [],
   "source": [
    "def gen_item_ot_feature(stat_df, extend_days):\n",
    "    stat_item_df = stat_df[['item_id', 'item_brand_id', 'item_category_id', 'item_city_id', 'context_day', 'context_hour', 'user_gender_id', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_item_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        dfs = {'all':stat_item_df.loc[stat_item_df['context_day'] != day],\n",
    "               'last':stat_item_df.loc[stat_item_df['context_day'] == day - 1]\n",
    "              }\n",
    "        item_features = dfs['all'].drop(columns=['is_trade']).drop_duplicates()\n",
    "        item_features.loc[:, 'context_day'] = day\n",
    "        groupbys = {'i' : ['item_id'], \n",
    "                    'ib' : ['item_brand_id'], \n",
    "                    'icg' : ['item_category_id'],\n",
    "                    'ict' : ['item_city_id'],\n",
    "                    'ih' : ['item_id', 'context_hour'], \n",
    "                    'ibh' : ['item_brand_id', 'context_hour'], \n",
    "                    'icgh' : ['item_category_id', 'context_hour'],\n",
    "                    'iug' : ['item_id', 'user_gender_id'],\n",
    "                    'ibug' : ['item_brand_id', 'user_gender_id'],\n",
    "                    'icug' : ['item_category_id', 'user_gender_id']}\n",
    "        for key, groupby in groupbys.items():\n",
    "            for dkey, item_df in dfs.items():\n",
    "                if item_df.empty:\n",
    "                    continue\n",
    "                key = dkey + '_' + key\n",
    "                trade_cnt_key = 'itrade_' + key + '_cnt'\n",
    "                query_cnt_key = 'iquery_' + key + '_cnt'\n",
    "                item_rate = item_df[groupby + ['is_trade']].rename(columns={'is_trade':trade_cnt_key})\n",
    "                item_rate.loc[:, query_cnt_key] = 1\n",
    "                item_rate = item_rate.groupby(groupby, as_index=False).sum()\n",
    "                item_rate.loc[:, 'irate_' + key] = item_rate[trade_cnt_key] / item_rate[query_cnt_key]\n",
    "                item_features = item_features.merge(item_rate, how='left')\n",
    "\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), item_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            item_features.loc[:,cnt_columns] = item_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        item_features = min_max_normalize_log(item_features, cnt_columns)\n",
    "        feature_frames.append(item_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "\n",
    "def gen_item_property_feature(df):\n",
    "    prop_item_df = df[['item_property_list']].drop_duplicates()\n",
    "    for i in range(5):\n",
    "        prop_item_df.loc[:,'property_%d'%(i)] = prop_item_df.loc[:,'item_property_list'].apply(\n",
    "            lambda x:x.split(\";\")[i] if len(x.split(\";\")) > i else \" \")\n",
    "    return prop_item_df\n",
    "\n",
    "\n",
    "def gen_item_prob_feature(stat_df, columns):\n",
    "    print('一个item有多少brand,price salse collected level……')\n",
    "    data = stat_df[['instance_id'] + columns]\n",
    "\n",
    "    itemcnt = data.groupby(['item_id'], as_index=False)['instance_id'].agg({'item_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['item_id'], how='left')\n",
    "\n",
    "    for col in ['item_brand_id','item_city_id', 'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level']:\n",
    "        itemcnt = data.groupby([col, 'item_id'], as_index=False)['instance_id'].agg({str(col) + '_item_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'item_id'], how='left')\n",
    "        data[str(col) + '_item_prob']=data[str(col) + '_item_cnt']/data['item_cnt']\n",
    "    del data['item_cnt']\n",
    "\n",
    "    print('一个brand有多少price salse collected level……')\n",
    "\n",
    "    itemcnt = data.groupby(['item_brand_id'], as_index=False)['instance_id'].agg({'item_brand_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['item_brand_id'], how='left')\n",
    "\n",
    "    for col in ['item_city_id', 'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level']:\n",
    "        itemcnt = data.groupby([col, 'item_brand_id'], as_index=False)['instance_id'].agg({str(col) + '_brand_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'item_brand_id'], how='left')\n",
    "        data[str(col) + '_brand_prob'] = data[str(col) + '_brand_cnt'] / data['item_brand_cnt']\n",
    "    del data['item_brand_cnt']\n",
    "\n",
    "    print('一个city有多少item_price_level，item_sales_level，item_collected_level，item_pv_level')\n",
    "\n",
    "    itemcnt = data.groupby(['item_city_id'], as_index=False)['instance_id'].agg({'item_city_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['item_city_id'], how='left')\n",
    "    for col in ['item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level']:\n",
    "        itemcnt = data.groupby([col, 'item_city_id'], as_index=False)['instance_id'].agg({str(col) + '_city_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'item_city_id'], how='left')\n",
    "        data[str(col) + '_city_prob'] = data[str(col) + '_city_cnt'] / data['item_city_cnt']\n",
    "    del data['item_city_cnt']\n",
    "\n",
    "    print('一个price有多少item_sales_level，item_collected_level，item_pv_level')\n",
    "\n",
    "    itemcnt = data.groupby(['item_price_level'], as_index=False)['instance_id'].agg({'item_price_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['item_price_level'], how='left')\n",
    "    for col in ['item_sales_level', 'item_collected_level', 'item_pv_level']:\n",
    "        itemcnt = data.groupby([col, 'item_city_id'], as_index=False)['instance_id'].agg({str(col) + '_price_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'item_city_id'], how='left')\n",
    "        data[str(col) + '_price_prob'] = data[str(col) + '_price_cnt'] / data['item_price_cnt']\n",
    "    del data['item_price_cnt']\n",
    "\n",
    "    print('一个item_sales_level有多少item_collected_level，item_pv_level')\n",
    "\n",
    "    itemcnt = data.groupby(['item_sales_level'], as_index=False)['instance_id'].agg({'item_salse_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['item_sales_level'], how='left')\n",
    "    for col in ['item_collected_level', 'item_pv_level']:\n",
    "        itemcnt = data.groupby([col, 'item_sales_level'], as_index=False)['instance_id'].agg({str(col) + '_salse_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'item_sales_level'], how='left')\n",
    "        data[str(col) + '_salse_prob'] = data[str(col) + '_salse_cnt'] / data['item_salse_cnt']\n",
    "    del data['item_salse_cnt']\n",
    "\n",
    "    print('一个item_collected_level有多少item_pv_level')\n",
    "\n",
    "    itemcnt = data.groupby(['item_collected_level'], as_index=False)['instance_id'].agg({'item_coll_cnt': 'count'})\n",
    "    data = pd.merge(data, itemcnt, on=['item_collected_level'], how='left')\n",
    "    for col in ['item_pv_level']:\n",
    "        itemcnt = data.groupby([col, 'item_collected_level'], as_index=False)['instance_id'].agg({str(col) + '_coll_cnt': 'count'})\n",
    "        data = pd.merge(data, itemcnt, on=[col, 'item_collected_level'], how='left')\n",
    "        data[str(col) + '_coll_prob'] = data[str(col) + '_coll_cnt'] / data['item_coll_cnt']\n",
    "    del data['item_coll_cnt']\n",
    "    \n",
    "    del data['instance_id']\n",
    "    \n",
    "    cnt_columns = filter(lambda x:x.endswith('_cnt'), data.columns.values)\n",
    "    data = min_max_normalize(data, cnt_columns)\n",
    "    return data\n",
    "\n",
    "def process_item_prob_feature(train_df, test_df, stat_df, all_df):\n",
    "    base_columns = ['item_id', 'item_brand_id','item_city_id', 'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level']\n",
    "    stat_item_df = gen_item_prob_feature(all_df, base_columns).drop_duplicates()\n",
    "    return map(lambda df:df[base_columns].merge(stat_item_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "def process_item_ot_feature(train_df, test_df, stat_df, all_df = None):\n",
    "    stat_item_df = gen_item_ot_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['item_id', 'item_brand_id', 'item_category_id', 'item_city_id', 'context_day', 'context_hour', 'user_gender_id']\n",
    "    return map(lambda df:df[base_columns].merge(stat_item_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "def process_item_property_feature(train_df, test_df, stat_df = None, all_df = None):\n",
    "    return map(gen_item_property_feature, (train_df, test_df))\n",
    "\n",
    "# train_item_ft, test_item_ft = process_item_ot_feature(train_df, test_df, stat_df)\n",
    "# train_item_ft, test_item_ft = process_item_property_feature(train_df, test_df, stat_df)\n",
    "# train_item_ft, test_item_ft = process_item_prob_feature(train_df, test_df, stat_df, all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立店铺特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_shop_ot_feature(stat_df, extend_days):\n",
    "    stat_shop_df = stat_df[['shop_id', 'context_day', 'context_hour', 'user_gender_id', 'user_age_level2', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_shop_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        dfs = {'all':stat_shop_df.loc[stat_shop_df['context_day'] != day],\n",
    "               'last':stat_shop_df.loc[stat_shop_df['context_day'] == day - 1]\n",
    "              }\n",
    "        shop_features = dfs['all'].drop(columns=['is_trade']).drop_duplicates()\n",
    "        shop_features.loc[:, 'context_day'] = day\n",
    "        groupbys = {'s' : ['shop_id'], \n",
    "                    'sh' : ['shop_id', 'context_hour'],\n",
    "                    'sug' : ['shop_id', 'user_gender_id'],\n",
    "                    'sua' : ['shop_id', 'user_age_level2']}\n",
    "        for key, groupby in groupbys.items():\n",
    "            for dkey, shop_df in dfs.items():\n",
    "                if shop_df.empty:\n",
    "                    continue\n",
    "                key = dkey + '_' + key\n",
    "                trade_cnt_key = 'strade_' + key + '_cnt'\n",
    "                query_cnt_key = 'squery_' + key + '_cnt'\n",
    "                shop_rate = shop_df[groupby + ['is_trade']].rename(columns={'is_trade':trade_cnt_key})\n",
    "                shop_rate.loc[:, query_cnt_key] = 1\n",
    "                shop_rate = shop_rate.groupby(groupby, as_index=False).sum()\n",
    "                shop_rate.loc[:, 'srate_' + key] = shop_rate[trade_cnt_key] / shop_rate[query_cnt_key]\n",
    "                shop_features = shop_features.merge(shop_rate, how='left')\n",
    "\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), shop_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            shop_features.loc[:,cnt_columns] = shop_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        shop_features = min_max_normalize_log(shop_features, cnt_columns)\n",
    "        feature_frames.append(shop_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "    \n",
    "    \n",
    "\n",
    "def process_shop_score_qcut_feature(train_df, test_df, stat_df=None, all_df=None):\n",
    "    tr_shop_df = train_df[['shop_id', 'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description']]\n",
    "    te_shop_df = test_df[['shop_id', 'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description']]\n",
    "    \n",
    "    a_shop_df = tr_shop_df.append(te_shop_df)\n",
    "    labels = map(lambda x:str(x), range(11))\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_review_positive_rate'], 24, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_review_positive_rate_qcut'] = pd.cut(tr_shop_df['shop_review_positive_rate'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_review_positive_rate_qcut'] = pd.cut(te_shop_df['shop_review_positive_rate'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_score_service'], 11, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_score_service_qcut'] = pd.cut(tr_shop_df['shop_score_service'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_score_service_qcut'] = pd.cut(te_shop_df['shop_score_service'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_score_delivery'], 11, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_score_delivery_qcut'] = pd.cut(tr_shop_df['shop_score_delivery'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_score_delivery_qcut'] = pd.cut(te_shop_df['shop_score_delivery'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_score_description'], 11, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_score_description_qcut'] = pd.cut(tr_shop_df['shop_score_description'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_score_description_qcut'] = pd.cut(te_shop_df['shop_score_description'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    return tr_shop_df.drop_duplicates(), te_shop_df.drop_duplicates()\n",
    "\n",
    "def process_shop_ot_feature(train_df, test_df, stat_df, all_df=None):\n",
    "    stat_shop_df = gen_shop_ot_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['shop_id', 'context_day', 'context_hour', 'user_gender_id', 'user_age_level2']\n",
    "    return map(lambda df:df[base_columns].merge(stat_shop_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "# train_shop_ct1, test_shop_ct1 = process_shop_score_qcut_feature(train_df, test_df)\n",
    "train_shop_ct2, test_shop_ct2 = process_shop_ot_feature(train_df, test_df, stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立上下文特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category_hit(row):\n",
    "    pre_list = row['predict_category_property'].split(';')\n",
    "    category_list = row['item_category_list'].split(';')\n",
    "    # start with second level category\n",
    "    ret = 0\n",
    "    for i in category_list[1:]:\n",
    "        for k in range(len(pre_list)):\n",
    "            if i in pre_list[k]:\n",
    "                # combime small datas.\n",
    "                if ret == 0 or k < ret:\n",
    "                    return 5 if k > 5 else k\n",
    "    return ret\n",
    "\n",
    "def gen_context_time_feature(df):\n",
    "    context_df = df[['user_id', 'item_id', 'context_id', 'context_timestamp', 'context_datetime', 'context_hour', 'context_page_id', 'context_day']]\n",
    "    # by time info\n",
    "    context_df.loc[:,'context_week'] = context_df.loc[:,'context_datetime'].map(lambda x:x.weekday())\n",
    "    context_df.loc[:,'context_minute'] = context_df.loc[:,'context_datetime'].map(lambda x:x.minute)\n",
    "    context_df.loc[:,'context_tmhour'] = context_df.loc[:,'context_hour'] + context_df.loc[:,'context_minute'] / 60\n",
    "    context_df.loc[:,'context_minute_5'] = context_df.loc[:,'context_minute'].map(lambda x:x / 5)\n",
    "    context_df.loc[:,'context_tmhour_5'] = context_df.loc[:,'context_tmhour'].map(lambda x:x / 5)\n",
    "    context_df.loc[:,'context_minute_20'] = context_df.loc[:,'context_minute'].map(lambda x:x / 20)\n",
    "    context_df.loc[:,'context_tmhour_20'] = context_df.loc[:,'context_tmhour'].map(lambda x:x / 20)\n",
    "    context_df.loc[:,'context_tmhour_sin'] = context_df.loc[:,'context_tmhour'].map(lambda x: math.sin((x-12)/24*2*math.pi))\n",
    "    context_df.loc[:,'context_tmhour_cos'] = context_df.loc[:,'context_tmhour'].map(lambda x: math.cos((x-12)/24*2*math.pi))\n",
    "    context_df.loc[:,'context_isworkday'] = context_df.loc[:,'context_week'].map(lambda x: 1 if x < 5 else 2)\n",
    "    \n",
    "    user_query_day = context_df[['user_id', 'context_day']]\n",
    "    user_query_day.loc[:,'u_day_query_cnt'] = 1\n",
    "    user_query_day = user_query_day.groupby(['user_id', 'context_day'], as_index=False).sum()\n",
    "\n",
    "    user_query_hour = context_df[['user_id', 'context_day', 'context_hour']]\n",
    "    user_query_hour.loc[:,'u_hour_query_cnt'] = 1\n",
    "    user_query_hour = user_query_hour.groupby(['user_id', 'context_day', 'context_hour'], as_index=False).sum()\n",
    "   \n",
    "    user_query_features = user_query_hour.merge(user_query_day)\n",
    "    cnt_columns = filter(lambda x:x.endswith('_cnt'), user_query_features.columns.values)\n",
    "    user_query_features = min_max_normalize(user_query_features, cnt_columns)\n",
    "    \n",
    "    item_query_day = context_df[['item_id', 'context_day']]\n",
    "    item_query_day.loc[:,'i_day_query_cnt'] = 1\n",
    "    item_query_day = item_query_day.groupby(['item_id', 'context_day'], as_index=False).sum()\n",
    "\n",
    "    item_query_hour = context_df[['item_id', 'context_day', 'context_hour']]\n",
    "    item_query_hour.loc[:,'i_hour_query_cnt'] = 1\n",
    "    item_query_hour = item_query_hour.groupby(['item_id', 'context_day', 'context_hour'], as_index=False).sum()\n",
    "   \n",
    "    item_query_features = item_query_hour.merge(item_query_day)\n",
    "    cnt_columns = filter(lambda x:x.endswith('_cnt'), item_query_features.columns.values)\n",
    "    item_query_features = min_max_normalize(item_query_features, cnt_columns)\n",
    "    \n",
    "    feature_frames = []\n",
    "    for name, day_df in context_df.groupby('context_day', as_index=False):\n",
    "        query_hour = day_df[['context_hour']]\n",
    "        query_hour.loc[:,'hour_query_cnt'] = 1\n",
    "        query_hour = query_hour.groupby('context_hour', as_index=False).sum()\n",
    "        query_hour.loc[:,'context_day'] = name\n",
    "        \n",
    "        query_features = query_hour.drop_duplicates()\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), query_features.columns.values)\n",
    "        query_features = min_max_normalize(query_features, cnt_columns)\n",
    "        feature_frames.append(query_features)\n",
    "        \n",
    "    query_features = pd.concat(feature_frames).drop_duplicates()\n",
    "    return context_df.merge(user_query_features).merge(item_query_features).merge(query_features).drop(columns=['context_datetime', 'context_timestamp']).drop_duplicates()\n",
    "\n",
    "def gen_context_predict_feature(df):\n",
    "    cp_df = df[['item_category_list', 'predict_category_property']]\n",
    "    frame = cp_df.apply(predict_category_hit, axis=1)\n",
    "    frame.name = 'category_predict_hit'\n",
    "    ret_df = df[['context_id']].join(frame)    \n",
    "    return ret_df\n",
    "\n",
    "def process_context_time_feature(train_df, test_df, stat_df=None, all_df=None):\n",
    "    return map(gen_context_time_feature, (train_df, test_df))\n",
    "\n",
    "def process_context_predict_feature(train_df, test_df, stat_df=None, all_df=None):\n",
    "    return map(gen_context_predict_feature, (train_df, test_df))\n",
    "\n",
    "# train_item_ct1, test_item_ct1 = process_context_time_feature(train_df, test_df)\n",
    "# train_item_ct2, test_item_ct2 = process_context_predict_feature(train_df, test_df)\n",
    "# TODO: 建立上下文预测属性数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_count_by_group(grp, args):\n",
    "    # 查询次数\n",
    "    by = args[0]\n",
    "    windows = args[1]\n",
    "    grp = grp.sort_values(\"context_timestamp\")\n",
    "    for index, row in grp.iterrows():\n",
    "        curr_date = row[\"context_timestamp\"]\n",
    "        for window in windows:\n",
    "            frame_name = by[:4] + '_qr_' + str(window) + '_cnt'\n",
    "            if window > 0:\n",
    "                grp_in_range = grp[(grp[\"context_timestamp\"] >= curr_date) & (\n",
    "                    grp[\"context_timestamp\"] < curr_date + window)]\n",
    "            else:\n",
    "                grp_in_range = grp[(grp[\"context_timestamp\"] < curr_date) & (\n",
    "                    grp[\"context_timestamp\"] >= curr_date + window)]\n",
    "            grp.at[index, frame_name] = grp_in_range.count()['context_id']# / float(window_size)\n",
    "    return grp.groupby([by, 'context_timestamp'], as_index=False)[map(lambda x:by[:4] + '_qr_' + str(x) + '_cnt', windows)].max()\n",
    "\n",
    "@timeit\n",
    "def query_rolling_rate(df, by, windows):\n",
    "    # 查询情况\n",
    "    grouped = df.groupby(by)\n",
    "    user_date_df = grouped.apply(rolling_count_by_group, [by, windows])\n",
    "\n",
    "    return user_date_df\n",
    "\n",
    "def gen_context_time_rolling_feature(df):\n",
    "    context_df = df[['user_id', 'item_id', 'context_id', 'context_timestamp', 'context_day']]\n",
    "\n",
    "    feature_frames = []\n",
    "    for name, day_df in context_df.groupby('context_day', as_index=False):\n",
    "        item_df = day_df[['item_id', 'context_id', 'context_timestamp', 'context_day']]\n",
    "        user_df = day_df[['user_id', 'context_id', 'context_timestamp', 'context_day']]\n",
    "        windows = [-300, 300, -1200, 1200, -3600, 3600]\n",
    "\n",
    "        i_query_rolling = query_rolling_rate(item_df, 'item_id', windows)\n",
    "        u_query_rolling = query_rolling_rate(user_df, 'user_id', windows)\n",
    "\n",
    "\n",
    "        query_features = i_query_rolling.merge(u_query_rolling).drop_duplicates()\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), query_features.columns.values)\n",
    "        query_features = min_max_normalize(query_features, cnt_columns)\n",
    "        feature_frames.append(query_features)\n",
    "\n",
    "    query_features = pd.concat(feature_frames).drop_duplicates()\n",
    "    return query_features\n",
    "\n",
    "def process_context_time_rolling_feature(train_df, test_df, stat_df=None, all_df=None):\n",
    "    return map(gen_context_time_rolling_feature, (train_df, test_df))\n",
    "\n",
    "# train_item_ctr, test_item_ctr = process_context_time_rolling_feature(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始处理特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(train_df), len(test_df)\n",
    "\n",
    "a = set(train_df['user_id'].tolist())\n",
    "b = set(test_df['user_id'].tolist())\n",
    "print 'train user count : %d, test user count : %d, both user count : %d' %(len(a), len(b), len(a & b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_test_features(tr_df, te_df, st_df, all_df, proc_func_list):\n",
    "    tr_fs, te_fs = process_base_feature(tr_df, te_df)\n",
    "\n",
    "    print 'begin base:', len(tr_fs), len(te_fs)\n",
    "    for proc_func in proc_func_list:\n",
    "        clear_output()\n",
    "        print 'processing ' + proc_func.func_name + ' ...'\n",
    "        tr_f, te_f = proc_func(tr_df, te_df, st_df, all_df)\n",
    "        clear_output()\n",
    "        print 'merging ' + proc_func.func_name + ':', len(tr_f), len(te_f)\n",
    "        tr_fs = tr_fs.merge(tr_f, how='left')\n",
    "        te_fs = te_fs.merge(te_f, how='left')\n",
    "    clear_output()\n",
    "    map(lambda x:x.drop_duplicates(inplace=True), (tr_fs, te_fs))\n",
    "    print 'done features:', len(tr_fs), len(te_fs)\n",
    "    return tr_fs, te_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fast features\n",
    "fast_proc_func_list = [process_base_combine_feature, process_intersection_id,\n",
    "                       process_context_predict_feature, process_context_time_feature,\n",
    "                       process_user_ot_feature, process_user_item_ot_feature,\n",
    "                       process_item_ot_feature, process_shop_ot_feature,\n",
    "                       process_item_property_feature, process_shop_score_qcut_feature,\n",
    "                       process_item_prob_feature, process_user_prob_feature, process_user_item_prob_feature]\n",
    "\n",
    "tr_fs, te_fs = process_train_test_features(train_df, test_df, stat_df, all_df, fast_proc_func_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow features\n",
    "user_old = True\n",
    "\n",
    "if user_old:\n",
    "    tr_fs_2 = pd.read_csv(cf.train_data_features_2_file_path, index_col=0)\n",
    "    te_fs_2 = pd.read_csv(cf.test_data_features_2_file_path, index_col=0)\n",
    "else:\n",
    "    slow_proc_func_list = [process_context_time_rolling_feature]\n",
    "    tr_fs_2, te_fs_2 = process_train_test_features(train_df, test_df, stat_df, all_df, slow_proc_func_list)\n",
    "    tr_fs_2.to_csv(cf.train_data_features_2_file_path)\n",
    "    te_fs_2.to_csv(cf.test_data_features_2_file_path)\n",
    "\n",
    "train_drop_columns = tr_fs_2.iloc[:,2:29].columns.values\n",
    "test_drop_columns = te_fs_2.iloc[:,1:28].columns.values\n",
    "tr_fs = tr_fs.merge(tr_fs_2.drop(columns=train_drop_columns), how='left')\n",
    "te_fs = te_fs.merge(te_fs_2.drop(columns=test_drop_columns), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print tr_fs.columns.values\n",
    "print tr_fs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_fs.to_csv(cf.train_data_features_file_path)\n",
    "te_fs.to_csv(cf.test_data_features_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
