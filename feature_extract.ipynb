{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels as sm\n",
    "import matplotlib.pylab as plt\n",
    "import config as cf\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from chinese_calendar import is_workday, is_holiday\n",
    "from jupyterthemes import jtplot\n",
    "from util import timeit\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "jtplot.style()\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(cf.round1_train_file_path, sep = ' ')\n",
    "test_df = pd.read_csv(cf.round1_test_file_path, sep = ' ')\n",
    "\n",
    "category_df = train_df['item_category_list'].unique()\n",
    "category_ids = pd.DataFrame({'item_category_list' : category_df, 'item_category_id' : np.arange(len(category_df))})\n",
    "train_df = train_df.merge(category_ids, on='item_category_list')\n",
    "test_df = test_df.merge(category_ids, on='item_category_list')\n",
    "\n",
    "time_offset = 8 * 60 * 60 - 365 * 24 * 60 * 60\n",
    "train_df.loc[:,'context_datetime'] = pd.to_datetime(train_df.loc[:,'context_timestamp'] + time_offset, unit='s')\n",
    "test_df.loc[:,'context_datetime'] = pd.to_datetime(test_df.loc[:,'context_timestamp'] + time_offset, unit='s')\n",
    "train_df.loc[:,'context_day'] = train_df.loc[:,'context_datetime'].map(lambda x:x.day)\n",
    "test_df.loc[:,'context_day'] = test_df.loc[:,'context_datetime'].map(lambda x:x.day)\n",
    "\n",
    "stat_df = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(df, name_list):\n",
    "    for name in name_list:\n",
    "        # 归一化\n",
    "        max_number = df[name].max()\n",
    "        min_number = df[name].min()\n",
    "        # assert max_number != min_number, 'max == min in COLUMN {0}'.format(name)\n",
    "        df.loc[:,name] = df.loc[:,name].map(lambda x: float(x - min_number + 1) / float(max_number - min_number + 1))\n",
    "        # 做简单的平滑,试试效果如何\n",
    "    return df\n",
    "\n",
    "def min_max_normalize_log(df, name_list):\n",
    "    for name in name_list:\n",
    "        # 归一化\n",
    "        max_number = df[name].max()\n",
    "        min_number = df[name].min()\n",
    "        # assert max_number != min_number, 'max == min in COLUMN {0}'.format(name)\n",
    "        df.loc[:,name] = df.loc[:,name].map(lambda x: np.log(x + 1) / np.log(max_number + 1))\n",
    "        # 做简单的平滑,试试效果如何\n",
    "    return df\n",
    "\n",
    "def normalize_log(df, name_list):\n",
    "    for name in name_list:\n",
    "        df.loc[:,name] = df.loc[:,name].map(lambda x: np.log(x + 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立基础特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_base_feature(df):\n",
    "    feature_list = []\n",
    "    if 'is_trade' in df:\n",
    "        feature_list.append('is_trade')\n",
    "    feature_list.extend(['instance_id', 'user_id', 'context_id', 'context_timestamp', 'context_day', 'item_property_list'])\n",
    "    feature_list.extend(['item_id', 'shop_id', 'item_brand_id', 'item_city_id', 'item_category_id'])\n",
    "    feature_list.extend(['item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level',\n",
    "                         'user_gender_id', 'user_age_level', 'user_occupation_id', 'user_star_level',\n",
    "                         'context_page_id', 'shop_review_num_level', 'shop_star_level',\n",
    "                         'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description'])\n",
    "    return df[feature_list]\n",
    "\n",
    "def process_base_feature(train_df, test_df, stat_df=None):\n",
    "    tr_df = gen_base_feature(train_df)\n",
    "    te_df = gen_base_feature(test_df)\n",
    "    return tr_df, te_df\n",
    "    \n",
    "# train_base_ft, test_base_ft = process_base_feature(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立用户特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_feature(df, extend_days):\n",
    "    user_df = df[['user_id', 'is_trade']]\n",
    "    \n",
    "    \n",
    "    stat_user_df = stat_df[['user_id', 'context_day', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_user_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        user_df = stat_user_df.loc[stat_user_df['context_day'] != day]\n",
    "\n",
    "        # user trade rate.\n",
    "        # user trade rate, group by catetory.\n",
    "        user_rate = user_df[['user_id', 'is_trade']].rename(columns={'is_trade':'u_trade_cnt'})\n",
    "        user_rate.loc[:,'user_cnt'] = 1\n",
    "        user_rate = user_rate.groupby('user_id', as_index=False).sum()\n",
    "        user_rate.loc[:, 'user_rate'] = user_rate['u_trade_cnt'] * 100 / user_rate['user_cnt']\n",
    "        \n",
    "        user_features = user_df.drop(columns=['is_trade']).drop_duplicates().merge(user_rate)\n",
    "        user_features.loc[:, 'context_day'] = day\n",
    "        if day not in stat_days:\n",
    "            cnt_columns = filter(lambda x:x.endswith('_cnt'), user_features.columns.values)\n",
    "            user_features.loc[:,cnt_columns] = user_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        \n",
    "        feature_frames.append(user_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "    \n",
    "    \n",
    "def process_user_feature(train_df, test_df, stat_df):\n",
    "    stat_user_df = gen_user_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['user_id', 'context_day']\n",
    "    return map(lambda df:df[base_columns].merge(stat_user_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "# train_user_ct2, test_user_ct2 = process_user_feature(train_df, test_df, stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立用户-商品特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_item_feature(stat_df, extend_days):\n",
    "    stat_user_item_df = stat_df[['user_id', 'item_id', 'item_brand_id', 'item_category_id', 'context_day', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_user_item_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        user_item_df = stat_user_item_df.loc[stat_user_item_df['context_day'] != day]\n",
    "        # user item trade rate.\n",
    "        user_item_rate = user_item_df[['user_id', 'item_id', 'is_trade']].rename(columns={'is_trade':'u_item_trade_cnt'})\n",
    "        user_item_rate.loc[:, 'u_item_cnt'] = 1\n",
    "        user_item_rate = user_item_rate.groupby(['user_id', 'item_id'], as_index=False).sum()\n",
    "        user_item_rate.loc[:, 'u_item_rate'] = user_item_rate['u_item_trade_cnt'] * 100 / user_item_rate['u_item_cnt']\n",
    "\n",
    "        user_brand_rate = user_item_df[['user_id', 'item_brand_id', 'is_trade']].rename(columns={'is_trade':'u_brand_trade_cnt'})\n",
    "        user_brand_rate.loc[:, 'u_brand_cnt'] = 1\n",
    "        user_brand_rate = user_brand_rate.groupby(['user_id', 'item_brand_id'], as_index=False).sum()\n",
    "        user_brand_rate.loc[:, 'u_brand_rate'] = user_brand_rate['u_brand_trade_cnt'] * 100 / user_brand_rate['u_brand_cnt']\n",
    "\n",
    "        # user item trade rate, group by catetory.\n",
    "        user_cate_rate = user_item_df[['user_id', 'item_category_id', 'is_trade']].rename(columns={'is_trade':'u_cate_trade_cnt'})\n",
    "        user_cate_rate.loc[:, 'u_cate_cnt'] = 1\n",
    "        user_cate_rate = user_cate_rate.groupby(['user_id', 'item_category_id'], as_index=False).sum()\n",
    "        user_cate_rate.loc[:, 'u_cate_rate'] = user_cate_rate['u_cate_trade_cnt'] * 100 / user_cate_rate['u_cate_cnt']\n",
    "\n",
    "        user_item_features = user_item_df.drop(columns=['is_trade']).drop_duplicates().merge(user_item_rate).merge(user_brand_rate).merge(user_cate_rate)\n",
    "        user_item_features.loc[:, 'context_day'] = day\n",
    "        \n",
    "        if day not in stat_days:\n",
    "            cnt_columns = filter(lambda x:x.endswith('_cnt'), user_item_features.columns.values)\n",
    "            user_item_features.loc[:,cnt_columns] = user_item_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        \n",
    "        feature_frames.append(user_item_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "\n",
    "def process_user_item_feature(train_df, test_df, stat_df):\n",
    "    stat_user_item_df = gen_user_item_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['user_id', 'item_id', 'item_brand_id', 'item_category_id', 'context_day']\n",
    "    return map(lambda df:df[base_columns].merge(stat_user_item_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "# train_user_ft, test_user_ft = process_user_item_feature(train_df, test_df, stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立商品特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_item_feature(stat_df, extend_days):\n",
    "    stat_item_df = stat_df[['item_id', 'item_brand_id', 'item_category_id', 'item_city_id', 'context_day', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_item_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        item_df = stat_item_df.loc[stat_item_df['context_day'] != day]\n",
    "    \n",
    "        item_rate = item_df[['item_id', 'is_trade']].rename(columns={'is_trade':'item_trade_cnt'})\n",
    "        item_rate.loc[:, 'item_cnt'] = 1\n",
    "        item_rate = item_rate.groupby('item_id', as_index=False).sum()\n",
    "        item_rate.loc[:, 'item_rate'] = item_rate['item_trade_cnt'] * 100 / item_rate['item_cnt']\n",
    "\n",
    "        item_brand_rate = item_df[['item_brand_id', 'is_trade']].rename(columns={'is_trade':'brand_trade_cnt'})\n",
    "        item_brand_rate.loc[:, 'brand_cnt'] = 1\n",
    "        item_brand_rate = item_brand_rate.groupby('item_brand_id', as_index=False).sum()\n",
    "        item_brand_rate.loc[:, 'brand_rate'] = item_brand_rate['brand_trade_cnt'] * 100 / item_brand_rate['brand_cnt']\n",
    "\n",
    "        item_cate_rate = item_df[['item_category_id', 'is_trade']].rename(columns={'is_trade':'cate_trade_cnt'})\n",
    "        item_cate_rate.loc[:, 'cate_cnt'] = 1\n",
    "        item_cate_rate = item_cate_rate.groupby('item_category_id', as_index=False).sum()\n",
    "        item_cate_rate.loc[:, 'cate_rate'] = item_cate_rate['cate_trade_cnt'] * 100 / item_cate_rate['cate_cnt']\n",
    "\n",
    "        item_city_rate = item_df[['item_city_id', 'is_trade']].rename(columns={'is_trade':'city_trade_cnt'})\n",
    "        item_city_rate.loc[:, 'city_cnt'] = 1\n",
    "        item_city_rate = item_city_rate.groupby('item_city_id', as_index=False).sum()\n",
    "        item_city_rate.loc[:, 'city_rate'] = item_city_rate['city_trade_cnt'] * 100/ item_city_rate['city_cnt']\n",
    "    \n",
    "        item_features = item_df.drop(columns=['is_trade']).drop_duplicates().merge(item_rate).merge(item_brand_rate).merge(item_cate_rate).merge(item_city_rate)\n",
    "        item_features.loc[:, 'context_day'] = day\n",
    "        \n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), item_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            item_features.loc[:,cnt_columns] = item_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        item_features = min_max_normalize_log(item_features, cnt_columns)\n",
    "        \n",
    "        feature_frames.append(item_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "\n",
    "def gen_item_property_feature(df):\n",
    "    prop_item_df = df[['item_property_list']].drop_duplicates()\n",
    "    for i in range(3):\n",
    "        prop_item_df.loc[:,'property_%d'%(i)] = prop_item_df.loc[:,'item_property_list'].apply(\n",
    "            lambda x:x.split(\";\")[i] if len(x.split(\";\")) > i else \" \")\n",
    "    return prop_item_df\n",
    "\n",
    "def process_item_feature(train_df, test_df, stat_df):\n",
    "    stat_item_df = gen_item_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['item_id', 'item_brand_id', 'item_category_id', 'item_city_id', 'context_day']\n",
    "    return map(lambda df:df[base_columns].merge(stat_item_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "def process_item_property_feature(train_df, test_df, stat_df = None):\n",
    "    return map(gen_item_property_feature, (train_df, test_df))\n",
    "\n",
    "# train_item_ft, test_item_ft = process_item_feature(train_df, test_df, stat_df)\n",
    "# train_item_ft, test_item_ft = process_item_property_feature(train_df, test_df, stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立上下文特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category_hit(row):\n",
    "    pre_list = row['predict_category_property'].split(';')\n",
    "    category_list = row['item_category_list'].split(';')\n",
    "    # start with second level category\n",
    "    ret = 0\n",
    "    for i in category_list[1:]:\n",
    "        for k in range(len(pre_list)):\n",
    "            if i in pre_list[k]:\n",
    "                # combime small datas.\n",
    "                if ret == 0 or k < ret:\n",
    "                    return 5 if k > 5 else k\n",
    "    return ret\n",
    "\n",
    "def gen_context_time_feature(df):\n",
    "    context_df = df[['user_id', 'item_id', 'context_id', 'context_timestamp', 'context_datetime', 'context_page_id', 'context_day']]\n",
    "    # by time info\n",
    "    context_df.loc[:,'context_hour'] = context_df.loc[:,'context_datetime'].map(lambda x:x.hour)\n",
    "    context_df.loc[:,'context_week'] = context_df.loc[:,'context_datetime'].map(lambda x:x.weekday())\n",
    "    context_df.loc[:,'context_minute'] = context_df.loc[:,'context_datetime'].map(lambda x:x.minute)\n",
    "    context_df.loc[:,'context_tmhour'] = context_df.loc[:,'context_hour'] + context_df.loc[:,'context_minute'] / 60\n",
    "    context_df.loc[:,'context_tmhour_sin'] = context_df.loc[:,'context_tmhour'].map(lambda x: math.sin((x-12)/24*2*math.pi))\n",
    "    context_df.loc[:,'context_tmhour_cos'] = context_df.loc[:,'context_tmhour'].map(lambda x: math.cos((x-12)/24*2*math.pi))\n",
    "    \n",
    "    user_query_day = context_df[['user_id', 'context_day']]\n",
    "    user_query_day.loc[:,'u_day_query_cnt'] = 1\n",
    "    user_query_day = user_query_day.groupby(['user_id', 'context_day'], as_index=False).sum()\n",
    "\n",
    "    user_query_hour = context_df[['user_id', 'context_day', 'context_hour']]\n",
    "    user_query_hour.loc[:,'u_hour_query_cnt'] = 1\n",
    "    user_query_hour = user_query_hour.groupby(['user_id', 'context_day', 'context_hour'], as_index=False).sum()\n",
    "   \n",
    "    user_query_features = user_query_hour.merge(user_query_day)\n",
    "    cnt_columns = filter(lambda x:x.endswith('_cnt'), user_query_features.columns.values)\n",
    "    user_query_features = min_max_normalize_log(user_query_features, cnt_columns)\n",
    "    \n",
    "    item_query_day = context_df[['item_id', 'context_day']]\n",
    "    item_query_day.loc[:,'i_day_query_cnt'] = 1\n",
    "    item_query_day = item_query_day.groupby(['item_id', 'context_day'], as_index=False).sum()\n",
    "\n",
    "    item_query_hour = context_df[['item_id', 'context_day', 'context_hour']]\n",
    "    item_query_hour.loc[:,'i_hour_query_cnt'] = 1\n",
    "    item_query_hour = item_query_hour.groupby(['item_id', 'context_day', 'context_hour'], as_index=False).sum()\n",
    "   \n",
    "    item_query_features = item_query_hour.merge(item_query_day)\n",
    "    cnt_columns = filter(lambda x:x.endswith('_cnt'), item_query_features.columns.values)\n",
    "    item_query_features = min_max_normalize_log(item_query_features, cnt_columns)\n",
    "    \n",
    "    feature_frames = []\n",
    "    for name, day_df in context_df.groupby('context_day', as_index=False):\n",
    "        query_hour = day_df[['context_hour']]\n",
    "        query_hour.loc[:,'hour_query_cnt'] = 1\n",
    "        query_hour = query_hour.groupby('context_hour', as_index=False).sum()\n",
    "        query_hour.loc[:,'context_day'] = name\n",
    "        \n",
    "        query_features = query_hour.drop_duplicates()\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), query_features.columns.values)\n",
    "        query_features = min_max_normalize(query_features, cnt_columns)\n",
    "        feature_frames.append(query_features)\n",
    "        \n",
    "    query_features = pd.concat(feature_frames).drop_duplicates()\n",
    "    return context_df.merge(user_query_features).merge(item_query_features).merge(query_features).drop(columns=['context_datetime', 'context_timestamp']).drop_duplicates()\n",
    "\n",
    "def gen_context_predict_feature(df):\n",
    "    cp_df = df[['item_category_list', 'predict_category_property']]\n",
    "    frame = cp_df.apply(predict_category_hit, axis=1)\n",
    "    frame.name = 'category_predict_hit'\n",
    "    ret_df = df[['context_id']].join(frame)    \n",
    "    return ret_df\n",
    "\n",
    "def process_context_time_feature(train_df, test_df, stat_df=None):\n",
    "    return map(gen_context_time_feature, (train_df, test_df))\n",
    "\n",
    "def process_context_predict_feature(train_df, test_df, stat_df=None):\n",
    "    return map(gen_context_predict_feature, (train_df, test_df))\n",
    "\n",
    "# train_item_ct1, test_item_ct1 = process_context_time_feature(train_df, test_df)\n",
    "# train_item_ct2, test_item_ct2 = process_context_predict_feature(train_df, test_df)\n",
    "# TODO: 建立上下文预测属性数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立店铺特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_shop_feature(df, extend_days):\n",
    "    shop_df = df[['shop_id', 'is_trade']]\n",
    "    \n",
    "    \n",
    "    stat_shop_df = stat_df[['shop_id', 'context_day', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_shop_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        shop_df = stat_shop_df.loc[stat_shop_df['context_day'] != day]\n",
    "\n",
    "        # shop trade rate.\n",
    "        # shop trade rate, group by catetory.\n",
    "        shop_rate = shop_df[['shop_id', 'is_trade']].rename(columns={'is_trade':'s_trade_cnt'})\n",
    "        shop_rate.loc[:,'shop_cnt'] = 1\n",
    "        shop_rate = shop_rate.groupby('shop_id', as_index=False).sum()\n",
    "        shop_rate.loc[:, 'shop_rate'] = shop_rate['s_trade_cnt'] * 100 / shop_rate['shop_cnt']\n",
    "\n",
    "        shop_features = shop_df.drop(columns=['is_trade']).drop_duplicates().merge(shop_rate)\n",
    "        shop_features.loc[:, 'context_day'] = day\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), shop_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            shop_features.loc[:,cnt_columns] = shop_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        shop_features = min_max_normalize_log(shop_features, cnt_columns)\n",
    "        \n",
    "        feature_frames.append(shop_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "    \n",
    "    \n",
    "\n",
    "def process_shop_score_qcut_feature(train_df, test_df, stat_df=None):\n",
    "    tr_shop_df = train_df[['shop_id', 'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description']]\n",
    "    te_shop_df = test_df[['shop_id', 'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description']]\n",
    "    \n",
    "    a_shop_df = tr_shop_df.append(te_shop_df)\n",
    "    labels = map(lambda x:str(x), range(11))\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_review_positive_rate'], 24, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_review_positive_rate_qcut'] = pd.cut(tr_shop_df['shop_review_positive_rate'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_review_positive_rate_qcut'] = pd.cut(te_shop_df['shop_review_positive_rate'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_score_service'], 11, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_score_service_qcut'] = pd.cut(tr_shop_df['shop_score_service'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_score_service_qcut'] = pd.cut(te_shop_df['shop_score_service'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_score_delivery'], 11, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_score_delivery_qcut'] = pd.cut(tr_shop_df['shop_score_delivery'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_score_delivery_qcut'] = pd.cut(te_shop_df['shop_score_delivery'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_score_description'], 11, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_score_description_qcut'] = pd.cut(tr_shop_df['shop_score_description'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_score_description_qcut'] = pd.cut(te_shop_df['shop_score_description'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    return tr_shop_df.drop_duplicates(), te_shop_df.drop_duplicates()\n",
    "\n",
    "def process_shop_feature(train_df, test_df, stat_df):\n",
    "    stat_shop_df = gen_shop_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['shop_id', 'context_day']\n",
    "    return map(lambda df:df[base_columns].merge(stat_shop_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "# train_shop_ct1, test_shop_ct1 = process_shop_score_qcut_feature(train_df, test_df)\n",
    "# train_shop_ct2, test_shop_ct2 = process_shop_feature(train_df, test_df, stat_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
