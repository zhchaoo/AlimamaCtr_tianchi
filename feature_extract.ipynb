{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels as sm\n",
    "import matplotlib.pylab as plt\n",
    "import config as cf\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from chinese_calendar import is_workday, is_holiday\n",
    "from jupyterthemes import jtplot\n",
    "from util import timeit\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "jtplot.style()\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(cf.round1_train_file_path, sep = ' ')\n",
    "test_df = pd.read_csv(cf.round1_test_file_path, sep = ' ')\n",
    "\n",
    "category_df = train_df['item_category_list'].unique()\n",
    "category_ids = pd.DataFrame({'item_category_list' : category_df, 'item_category_id' : np.arange(len(category_df))})\n",
    "train_df = train_df.merge(category_ids, on='item_category_list')\n",
    "test_df = test_df.merge(category_ids, on='item_category_list')\n",
    "\n",
    "time_offset = 8 * 60 * 60 - 365 * 24 * 60 * 60\n",
    "train_df.loc[:,'context_datetime'] = pd.to_datetime(train_df.loc[:,'context_timestamp'] + time_offset, unit='s')\n",
    "test_df.loc[:,'context_datetime'] = pd.to_datetime(test_df.loc[:,'context_timestamp'] + time_offset, unit='s')\n",
    "train_df.loc[:,'context_day'] = train_df.loc[:,'context_datetime'].map(lambda x:x.day)\n",
    "test_df.loc[:,'context_day'] = test_df.loc[:,'context_datetime'].map(lambda x:x.day)\n",
    "train_df.loc[:,'context_hour'] = train_df.loc[:,'context_datetime'].map(lambda x:x.hour)\n",
    "test_df.loc[:,'context_hour'] = test_df.loc[:,'context_datetime'].map(lambda x:x.hour)\n",
    "\n",
    "stat_df = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(df, name_list):\n",
    "    for name in name_list:\n",
    "        # 归一化\n",
    "        max_number = df[name].max()\n",
    "        min_number = df[name].min()\n",
    "        # assert max_number != min_number, 'max == min in COLUMN {0}'.format(name)\n",
    "        df.loc[:,name] = df.loc[:,name].map(lambda x: float(x - min_number + 0.001) / float(max_number - min_number + 0.001))\n",
    "        # 做简单的平滑,试试效果如何\n",
    "    return df\n",
    "\n",
    "def min_max_normalize_log(df, name_list):\n",
    "    for name in name_list:\n",
    "        # 归一化\n",
    "        max_number = df[name].max()\n",
    "        min_number = df[name].min()\n",
    "        # assert max_number != min_number, 'max == min in COLUMN {0}'.format(name)\n",
    "        df.loc[:,name] = df.loc[:,name].map(lambda x: np.log(x + 1) / np.log(max_number + 1))\n",
    "        # 做简单的平滑,试试效果如何\n",
    "    return df\n",
    "\n",
    "def normalize_log(df, name_list):\n",
    "    for name in name_list:\n",
    "        df.loc[:,name] = df.loc[:,name].map(lambda x: np.log(x + 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立基础特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_base_feature(df):\n",
    "    feature_list = []\n",
    "    if 'is_trade' in df:\n",
    "        feature_list.append('is_trade')\n",
    "    feature_list.extend(['instance_id', 'user_id', 'context_id', 'context_timestamp', 'context_day', 'item_property_list'])\n",
    "    feature_list.extend(['item_id', 'shop_id', 'item_brand_id', 'item_city_id', 'item_category_id'])\n",
    "    feature_list.extend(['item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level',\n",
    "                         'user_gender_id', 'user_age_level', 'user_occupation_id', 'user_star_level',\n",
    "                         'context_page_id', 'shop_review_num_level', 'shop_star_level',\n",
    "                         'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description'])\n",
    "    base_feature = df[feature_list]\n",
    "    base_feature.loc[:,'is_strong_age'] = base_feature['user_age_level'].apply(lambda x: 1 if x == 1004 or x == 1005 or x == 1006 or x == 1007  else 2)\n",
    "    base_feature.loc[:,'user_start_level2'] = base_feature['user_star_level'].apply(lambda x: 1 if x == -1 or x == 3000 or x == 3001 else 3 if x == 3010 else 2)\n",
    "    return base_feature\n",
    "\n",
    "def gen_base_combine_feature(df):\n",
    "    base_df = df[['item_id', 'user_id', 'shop_id', 'item_sales_level', 'item_price_level', 'item_collected_level',\n",
    "                 'user_gender_id', 'user_age_level', 'user_star_level', 'user_occupation_id',\n",
    "                 'shop_review_num_level', 'shop_star_level']].drop_duplicates()\n",
    "    combine_list = {\n",
    "        'price_sale': ['item_sales_level', 'item_price_level'],\n",
    "        'collect_sale': ['item_sales_level', 'item_collected_level'],\n",
    "        'collect_price': ['item_price_level', 'item_collected_level'],\n",
    "        'gender_age': ['user_gender_id', 'user_age_level'],\n",
    "        'gender_occ': ['user_gender_id', 'user_occupation_id'],\n",
    "        'gender_star': ['user_gender_id', 'user_star_level'],\n",
    "        'review_star': ['shop_review_num_level', 'shop_star_level']\n",
    "    }\n",
    "    for key, combine in combine_list.items():\n",
    "        base_df.loc[:,key] = base_df[combine[0]] * 24 + base_df[combine[1]]\n",
    "    \n",
    "    return base_df\n",
    "    \n",
    "def process_base_feature(train_df, test_df, stat_df=None):\n",
    "    tr_df = gen_base_feature(train_df)\n",
    "    te_df = gen_base_feature(test_df)\n",
    "    return tr_df, te_df\n",
    "\n",
    "def process_base_combine_feature(train_df, test_df, stat_df=None):\n",
    "    return map(gen_base_combine_feature, (train_df, test_df))\n",
    "\n",
    "# train_base_ft, test_base_ft = process_base_feature(train_df, test_df)\n",
    "# train_base_ft1, test_base_ft1 = process_base_combine_feature(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立用户特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_feature(stat_df, extend_days):\n",
    "    stat_user_df = stat_df[['user_id', 'context_day', 'context_hour', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_user_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        user_df = stat_user_df.loc[stat_user_df['context_day'] != day]\n",
    "        groupbys = {'u' : ['user_id'], \n",
    "                    'uh' : ['user_id', 'context_hour']}\n",
    "                    \n",
    "        user_features = user_df.drop(columns=['is_trade']).drop_duplicates()\n",
    "        user_features.loc[:, 'context_day'] = day\n",
    "        for key, groupby in groupbys.items():\n",
    "            trade_cnt_key = 'utrade_' + key + '_cnt'\n",
    "            query_cnt_key = 'uquery_' + key + '_cnt'\n",
    "            user_rate = user_df[groupby + ['is_trade']].rename(columns={'is_trade':trade_cnt_key})\n",
    "            user_rate.loc[:, query_cnt_key] = 1\n",
    "            user_rate = user_rate.groupby(groupby, as_index=False).sum()\n",
    "            user_rate.loc[:, 'urate_' + key] = user_rate[trade_cnt_key] / user_rate[query_cnt_key]\n",
    "            user_features = user_features.merge(user_rate)\n",
    "        \n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), user_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            user_features.loc[:,cnt_columns] = user_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        user_features = min_max_normalize_log(user_features, cnt_columns)\n",
    "        feature_frames.append(user_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "\n",
    "\n",
    "def process_user_intersection_id(train_df, test_df, stat_df = None):\n",
    "    a = set(train_df['user_id'].tolist())\n",
    "    b = set(test_df['user_id'].tolist())\n",
    "    user_is_id = a & b\n",
    "    tr_df = train_df.loc[train_df['user_id'].isin(user_is_id)][['user_id']]\n",
    "    tr_df.loc[:, 'user_is_id'] = tr_df['user_id']\n",
    "    te_df = test_df.loc[test_df['user_id'].isin(user_is_id)][['user_id']]\n",
    "    te_df.loc[:, 'user_is_id'] = te_df['user_id']\n",
    "    return tr_df, te_df\n",
    "\n",
    "\n",
    "def process_user_feature(train_df, test_df, stat_df):\n",
    "    stat_user_df = gen_user_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['user_id', 'context_day']\n",
    "    return map(lambda df:df[base_columns].merge(stat_user_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "\n",
    "# train_user_ct2, test_user_ct2 = process_user_feature(train_df, test_df, stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立用户-商品特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_item_feature(stat_df, extend_days):\n",
    "    stat_user_item_df = stat_df[['user_id', 'item_id', 'item_brand_id', 'item_category_id', 'item_city_id', 'item_price_level', 'context_day', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_user_item_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        user_item_df = stat_user_item_df.loc[stat_user_item_df['context_day'] != day]\n",
    "        groupbys = {'ui' : ['user_id', 'item_id'], \n",
    "                    'ub' : ['user_id', 'item_brand_id'], \n",
    "                    'ucg' : ['user_id', 'item_category_id'],\n",
    "                    'uct' : ['user_id', 'item_city_id'],\n",
    "                    'uip' : ['user_id', 'item_price_level']}\n",
    "                    \n",
    "        user_item_features = user_item_df.drop(columns=['is_trade']).drop_duplicates()\n",
    "        user_item_features.loc[:, 'context_day'] = day\n",
    "        for key, groupby in groupbys.items():\n",
    "            trade_cnt_key = 'uitrade_' + key + '_cnt'\n",
    "            query_cnt_key = 'uiquery_' + key + '_cnt'\n",
    "            user_item_rate = user_item_df[groupby + ['is_trade']].rename(columns={'is_trade':trade_cnt_key})\n",
    "            user_item_rate.loc[:, query_cnt_key] = 1\n",
    "            user_item_rate = user_item_rate.groupby(groupby, as_index=False).sum()\n",
    "            user_item_rate.loc[:, 'uirate_' + key] = user_item_rate[trade_cnt_key] / user_item_rate[query_cnt_key]\n",
    "            user_item_features = user_item_features.merge(user_item_rate)\n",
    "        \n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), user_item_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            user_item_features.loc[:,cnt_columns] = user_item_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        user_item_features = min_max_normalize_log(user_item_features, cnt_columns)\n",
    "        feature_frames.append(user_item_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "\n",
    "def process_user_item_feature(train_df, test_df, stat_df):\n",
    "    stat_user_item_df = gen_user_item_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['user_id', 'item_id', 'item_brand_id', 'item_category_id', 'context_day']\n",
    "    return map(lambda df:df[base_columns].merge(stat_user_item_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "# train_user_ft, test_user_ft = process_user_item_feature(train_df, test_df, stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立商品特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_item_feature(stat_df, extend_days):\n",
    "    stat_item_df = stat_df[['item_id', 'item_brand_id', 'item_category_id', 'item_city_id', 'context_day', 'context_hour', 'user_gender_id', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_item_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        item_df = stat_item_df.loc[stat_item_df['context_day'] != day]\n",
    "        groupbys = {'i' : ['item_id'], \n",
    "                    'ib' : ['item_brand_id'], \n",
    "                    'icg' : ['item_category_id'],\n",
    "                    'ict' : ['item_city_id'],\n",
    "                    'ih' : ['item_id', 'context_hour'], \n",
    "                    'ibh' : ['item_brand_id', 'context_hour'], \n",
    "                    'icgh' : ['item_category_id', 'context_hour'],\n",
    "                    'iug' : ['item_id', 'user_gender_id'],\n",
    "                    'ibug' : ['item_brand_id', 'user_gender_id'],\n",
    "                    'icug' : ['item_category_id', 'user_gender_id']}\n",
    "                    \n",
    "        item_features = item_df.drop(columns=['is_trade']).drop_duplicates()\n",
    "        item_features.loc[:, 'context_day'] = day\n",
    "        for key, groupby in groupbys.items():\n",
    "            trade_cnt_key = 'itrade_' + key + '_cnt'\n",
    "            query_cnt_key = 'iquery_' + key + '_cnt'\n",
    "            item_rate = item_df[groupby + ['is_trade']].rename(columns={'is_trade':trade_cnt_key})\n",
    "            item_rate.loc[:, query_cnt_key] = 1\n",
    "            item_rate = item_rate.groupby(groupby, as_index=False).sum()\n",
    "            item_rate.loc[:, 'irate_' + key] = item_rate[trade_cnt_key] / item_rate[query_cnt_key]\n",
    "            item_features = item_features.merge(item_rate)\n",
    "        \n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), item_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            item_features.loc[:,cnt_columns] = item_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        item_features = min_max_normalize_log(item_features, cnt_columns)\n",
    "        feature_frames.append(item_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "\n",
    "def gen_item_property_feature(df):\n",
    "    prop_item_df = df[['item_property_list']].drop_duplicates()\n",
    "    for i in range(5):\n",
    "        prop_item_df.loc[:,'property_%d'%(i)] = prop_item_df.loc[:,'item_property_list'].apply(\n",
    "            lambda x:x.split(\";\")[i] if len(x.split(\";\")) > i else \" \")\n",
    "    return prop_item_df\n",
    "\n",
    "def process_item_feature(train_df, test_df, stat_df):\n",
    "    stat_item_df = gen_item_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['item_id', 'item_brand_id', 'item_category_id', 'item_city_id', 'context_day']\n",
    "    return map(lambda df:df[base_columns].merge(stat_item_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "def process_item_property_feature(train_df, test_df, stat_df = None):\n",
    "    return map(gen_item_property_feature, (train_df, test_df))\n",
    "\n",
    "# train_item_ft, test_item_ft = process_item_feature(train_df, test_df, stat_df)\n",
    "# train_item_ft, test_item_ft = process_item_property_feature(train_df, test_df, stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立上下文特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category_hit(row):\n",
    "    pre_list = row['predict_category_property'].split(';')\n",
    "    category_list = row['item_category_list'].split(';')\n",
    "    # start with second level category\n",
    "    ret = 0\n",
    "    for i in category_list[1:]:\n",
    "        for k in range(len(pre_list)):\n",
    "            if i in pre_list[k]:\n",
    "                # combime small datas.\n",
    "                if ret == 0 or k < ret:\n",
    "                    return 5 if k > 5 else k\n",
    "    return ret\n",
    "\n",
    "def gen_context_time_feature(df):\n",
    "    context_df = df[['user_id', 'item_id', 'context_id', 'context_timestamp', 'context_datetime', 'context_hour', 'context_page_id', 'context_day']]\n",
    "    # by time info\n",
    "    context_df.loc[:,'context_week'] = context_df.loc[:,'context_datetime'].map(lambda x:x.weekday())\n",
    "    context_df.loc[:,'context_minute'] = context_df.loc[:,'context_datetime'].map(lambda x:x.minute)\n",
    "    context_df.loc[:,'context_tmhour'] = context_df.loc[:,'context_hour'] + context_df.loc[:,'context_minute'] / 60\n",
    "    context_df.loc[:,'context_minute_5'] = context_df.loc[:,'context_minute'].map(lambda x:x / 5)\n",
    "    context_df.loc[:,'context_tmhour_5'] = context_df.loc[:,'context_tmhour'].map(lambda x:x / 5)\n",
    "    context_df.loc[:,'context_minute_20'] = context_df.loc[:,'context_minute'].map(lambda x:x / 20)\n",
    "    context_df.loc[:,'context_tmhour_20'] = context_df.loc[:,'context_tmhour'].map(lambda x:x / 20)\n",
    "    context_df.loc[:,'context_tmhour_sin'] = context_df.loc[:,'context_tmhour'].map(lambda x: math.sin((x-12)/24*2*math.pi))\n",
    "    context_df.loc[:,'context_tmhour_cos'] = context_df.loc[:,'context_tmhour'].map(lambda x: math.cos((x-12)/24*2*math.pi))\n",
    "    context_df.loc[:,'context_isworkday'] = context_df.loc[:,'context_week'].map(lambda x: 1 if x < 5 else 2)\n",
    "    \n",
    "    user_query_day = context_df[['user_id', 'context_day']]\n",
    "    user_query_day.loc[:,'u_day_query_cnt'] = 1\n",
    "    user_query_day = user_query_day.groupby(['user_id', 'context_day'], as_index=False).sum()\n",
    "\n",
    "    user_query_hour = context_df[['user_id', 'context_day', 'context_hour']]\n",
    "    user_query_hour.loc[:,'u_hour_query_cnt'] = 1\n",
    "    user_query_hour = user_query_hour.groupby(['user_id', 'context_day', 'context_hour'], as_index=False).sum()\n",
    "   \n",
    "    user_query_features = user_query_hour.merge(user_query_day)\n",
    "    cnt_columns = filter(lambda x:x.endswith('_cnt'), user_query_features.columns.values)\n",
    "    user_query_features = min_max_normalize(user_query_features, cnt_columns)\n",
    "    \n",
    "    item_query_day = context_df[['item_id', 'context_day']]\n",
    "    item_query_day.loc[:,'i_day_query_cnt'] = 1\n",
    "    item_query_day = item_query_day.groupby(['item_id', 'context_day'], as_index=False).sum()\n",
    "\n",
    "    item_query_hour = context_df[['item_id', 'context_day', 'context_hour']]\n",
    "    item_query_hour.loc[:,'i_hour_query_cnt'] = 1\n",
    "    item_query_hour = item_query_hour.groupby(['item_id', 'context_day', 'context_hour'], as_index=False).sum()\n",
    "   \n",
    "    item_query_features = item_query_hour.merge(item_query_day)\n",
    "    cnt_columns = filter(lambda x:x.endswith('_cnt'), item_query_features.columns.values)\n",
    "    item_query_features = min_max_normalize(item_query_features, cnt_columns)\n",
    "    \n",
    "    feature_frames = []\n",
    "    for name, day_df in context_df.groupby('context_day', as_index=False):\n",
    "        query_hour = day_df[['context_hour']]\n",
    "        query_hour.loc[:,'hour_query_cnt'] = 1\n",
    "        query_hour = query_hour.groupby('context_hour', as_index=False).sum()\n",
    "        query_hour.loc[:,'context_day'] = name\n",
    "        \n",
    "        query_features = query_hour.drop_duplicates()\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), query_features.columns.values)\n",
    "        query_features = min_max_normalize(query_features, cnt_columns)\n",
    "        feature_frames.append(query_features)\n",
    "        \n",
    "    query_features = pd.concat(feature_frames).drop_duplicates()\n",
    "    return context_df.merge(user_query_features).merge(item_query_features).merge(query_features).drop(columns=['context_datetime', 'context_timestamp']).drop_duplicates()\n",
    "\n",
    "def gen_context_predict_feature(df):\n",
    "    cp_df = df[['item_category_list', 'predict_category_property']]\n",
    "    frame = cp_df.apply(predict_category_hit, axis=1)\n",
    "    frame.name = 'category_predict_hit'\n",
    "    ret_df = df[['context_id']].join(frame)    \n",
    "    return ret_df\n",
    "\n",
    "def process_context_time_feature(train_df, test_df, stat_df=None):\n",
    "    return map(gen_context_time_feature, (train_df, test_df))\n",
    "\n",
    "def process_context_predict_feature(train_df, test_df, stat_df=None):\n",
    "    return map(gen_context_predict_feature, (train_df, test_df))\n",
    "\n",
    "# train_item_ct1, test_item_ct1 = process_context_time_feature(train_df, test_df)\n",
    "# train_item_ct2, test_item_ct2 = process_context_predict_feature(train_df, test_df)\n",
    "# TODO: 建立上下文预测属性数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_count_by_group(grp, args):\n",
    "    # 查询次数\n",
    "    by = args[0]\n",
    "    windows = args[1]\n",
    "    grp = grp.sort_values(\"context_timestamp\")\n",
    "    for index, row in grp.iterrows():\n",
    "        curr_date = row[\"context_timestamp\"]\n",
    "        for window in windows:\n",
    "            frame_name = by[:4] + '_qr_' + str(window) + '_cnt'\n",
    "            if window > 0:\n",
    "                grp_in_range = grp[(grp[\"context_timestamp\"] >= curr_date) & (\n",
    "                    grp[\"context_timestamp\"] < curr_date + window)]\n",
    "            else:\n",
    "                grp_in_range = grp[(grp[\"context_timestamp\"] <= curr_date) & (\n",
    "                    grp[\"context_timestamp\"] > curr_date + window)]\n",
    "            grp.at[index, frame_name] = grp_in_range.count()['context_id']# / float(window_size)\n",
    "    return grp.groupby([by, 'context_timestamp'], as_index=False)[map(lambda x:by[:4] + '_qr_' + str(x) + '_cnt', windows)].max()\n",
    "\n",
    "@timeit\n",
    "def query_rolling_rate(df, by, windows):\n",
    "    # 查询情况\n",
    "    grouped = df.groupby(by)\n",
    "    user_date_df = grouped.apply(rolling_count_by_group, [by, windows])\n",
    "\n",
    "    return user_date_df\n",
    "\n",
    "def gen_context_time_rolling_feature(df):\n",
    "    context_df = df[['user_id', 'item_id', 'context_id', 'context_timestamp', 'context_day']]\n",
    "\n",
    "    feature_frames = []\n",
    "    for name, day_df in context_df.groupby('context_day', as_index=False):\n",
    "        item_df = day_df[['item_id', 'context_id', 'context_timestamp', 'context_day']]\n",
    "        user_df = day_df[['user_id', 'context_id', 'context_timestamp', 'context_day']]\n",
    "        windows = [-10, 10, -30, 30, -60, 60]\n",
    "\n",
    "        i_query_rolling = query_rolling_rate(item_df, 'item_id', windows)\n",
    "        u_query_rolling = query_rolling_rate(user_df, 'user_id', windows)\n",
    "\n",
    "\n",
    "        query_features = i_query_rolling.merge(u_query_rolling).drop_duplicates()\n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), query_features.columns.values)\n",
    "        query_features = min_max_normalize(query_features, cnt_columns)\n",
    "        feature_frames.append(query_features)\n",
    "\n",
    "    query_features = pd.concat(feature_frames).drop_duplicates()\n",
    "    return query_features\n",
    "\n",
    "def process_context_time_rolling_feature(train_df, test_df, stat_df=None):\n",
    "    return map(gen_context_time_rolling_feature, (train_df, test_df))\n",
    "\n",
    "# train_item_ctr, test_item_ctr = process_context_time_rolling_feature(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立店铺特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_shop_feature(stat_df, extend_days):\n",
    "    stat_shop_df = stat_df[['shop_id', 'context_day', 'context_hour', 'user_gender_id', 'is_trade']]\n",
    "    \n",
    "    feature_frames = []\n",
    "    stat_days = set(stat_shop_df['context_day'].unique())\n",
    "    extend_days = set(extend_days)\n",
    "    for day in stat_days | extend_days:\n",
    "        shop_df = stat_shop_df.loc[stat_shop_df['context_day'] != day]\n",
    "        groupbys = {'s' : ['shop_id'], \n",
    "                    'sh' : ['shop_id', 'context_hour'],\n",
    "                    'sug' : ['shop_id', 'user_gender_id']}\n",
    "                    \n",
    "        shop_features = shop_df.drop(columns=['is_trade']).drop_duplicates()\n",
    "        shop_features.loc[:, 'context_day'] = day\n",
    "        for key, groupby in groupbys.items():\n",
    "            trade_cnt_key = 'strade_' + key + '_cnt'\n",
    "            query_cnt_key = 'squery_' + key + '_cnt'\n",
    "            shop_rate = shop_df[groupby + ['is_trade']].rename(columns={'is_trade':trade_cnt_key})\n",
    "            shop_rate.loc[:, query_cnt_key] = 1\n",
    "            shop_rate = shop_rate.groupby(groupby, as_index=False).sum()\n",
    "            shop_rate.loc[:, 'srate_' + key] = shop_rate[trade_cnt_key] / shop_rate[query_cnt_key]\n",
    "            shop_features = shop_features.merge(shop_rate)\n",
    "        \n",
    "        cnt_columns = filter(lambda x:x.endswith('_cnt'), shop_features.columns.values)\n",
    "        if day not in stat_days:\n",
    "            shop_features.loc[:,cnt_columns] = shop_features.loc[:,cnt_columns] * (len(stat_days) - 1) / len(stat_days)\n",
    "        shop_features = min_max_normalize_log(shop_features, cnt_columns)\n",
    "        feature_frames.append(shop_features)\n",
    "        \n",
    "    return pd.concat(feature_frames).drop_duplicates()\n",
    "    \n",
    "    \n",
    "\n",
    "def process_shop_score_qcut_feature(train_df, test_df, stat_df=None):\n",
    "    tr_shop_df = train_df[['shop_id', 'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description']]\n",
    "    te_shop_df = test_df[['shop_id', 'shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description']]\n",
    "    \n",
    "    a_shop_df = tr_shop_df.append(te_shop_df)\n",
    "    labels = map(lambda x:str(x), range(11))\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_review_positive_rate'], 24, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_review_positive_rate_qcut'] = pd.cut(tr_shop_df['shop_review_positive_rate'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_review_positive_rate_qcut'] = pd.cut(te_shop_df['shop_review_positive_rate'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_score_service'], 11, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_score_service_qcut'] = pd.cut(tr_shop_df['shop_score_service'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_score_service_qcut'] = pd.cut(te_shop_df['shop_score_service'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_score_delivery'], 11, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_score_delivery_qcut'] = pd.cut(tr_shop_df['shop_score_delivery'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_score_delivery_qcut'] = pd.cut(te_shop_df['shop_score_delivery'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    _, bins = pd.qcut(a_shop_df['shop_score_description'], 11, retbins=True, duplicates='drop')\n",
    "    tr_shop_df.loc[:,'shop_score_description_qcut'] = pd.cut(tr_shop_df['shop_score_description'], bins=bins, labels=labels).astype(int)\n",
    "    te_shop_df.loc[:,'shop_score_description_qcut'] = pd.cut(te_shop_df['shop_score_description'], bins=bins, labels=labels).astype(int)\n",
    "    \n",
    "    return tr_shop_df.drop_duplicates(), te_shop_df.drop_duplicates()\n",
    "\n",
    "def process_shop_feature(train_df, test_df, stat_df):\n",
    "    stat_shop_df = gen_shop_feature(stat_df, test_df['context_day'].unique())\n",
    "    base_columns = ['shop_id', 'context_day']\n",
    "    return map(lambda df:df[base_columns].merge(stat_shop_df).drop_duplicates(), (train_df, test_df))\n",
    "\n",
    "# train_shop_ct1, test_shop_ct1 = process_shop_score_qcut_feature(train_df, test_df)\n",
    "# train_shop_ct2, test_shop_ct2 = process_shop_feature(train_df, test_df, stat_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
